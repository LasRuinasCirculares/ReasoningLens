{
  "ASyMOB: Algebraic Symbolic Mathematical Operations Benchmark": "@misc{shalyt2025asymobalgebraicsymbolicmathematical,\n  archivePrefix = {arXiv},\n  author = {Michael Shalyt and Rotem Elimelech and Ido Kaminer},\n  eprint = {2505.23851},\n  primaryClass = {cs.CL},\n  title = {ASyMOB: Algebraic Symbolic Mathematical Operations Benchmark},\n  url = {https://arxiv.org/abs/2505.23851},\n  year = {2025}\n}",
  "C3oT: Generating Shorter Chain-of-Thought without Compromising Effectiveness": "@article{Kang_2025,\n  author = {Kang, Yu and Sun, Xianghui and Chen, Liangyu and Zou, Wei},\n  DOI = {10.1609/aaai.v39i23.34608},\n  ISSN = {2159-5399},\n  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},\n  month = apr,\n  number = {23},\n  pages = {24312–24320},\n  publisher = {Association for the Advancement of Artificial Intelligence (AAAI)},\n  title = {C3oT: Generating Shorter Chain-of-Thought Without Compromising Effectiveness},\n  url = {http://dx.doi.org/10.1609/aaai.v39i23.34608},\n  volume = {39},\n  year = {2025}\n}",
  "CODI: Compressing Chain-of-Thought into Continuous Space via Self-Distillation": "@inproceedings{Shen_2025,\n  author = {Shen, Zhenyi and Yan, Hanqi and Zhang, Linhai and Hu, Zhanghao and Du, Yali and He, Yulan},\n  booktitle = {Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing},\n  DOI = {10.18653/v1/2025.emnlp-main.36},\n  pages = {677–693},\n  publisher = {Association for Computational Linguistics},\n  title = {CODI: Compressing Chain-of-Thought into Continuous Space via Self-Distillation},\n  url = {http://dx.doi.org/10.18653/v1/2025.emnlp-main.36},\n  year = {2025}\n}",
  "Can Large Language Models Express Uncertainty Like Human?": "@article{Lecun_1998,\n  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},\n  DOI = {10.1109/5.726791},\n  ISSN = {0018-9219},\n  journal = {Proceedings of the IEEE},\n  number = {11},\n  pages = {2278–2324},\n  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},\n  title = {Gradient-based learning applied to document recognition},\n  url = {http://dx.doi.org/10.1109/5.726791},\n  volume = {86},\n  year = {1998}\n}",
  "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models": "@misc{kojima2023largelanguagemodelszeroshot,\n  archivePrefix = {arXiv},\n  author = {Takeshi Kojima and Shixiang Shane Gu and Machel Reid and Yutaka Matsuo and Yusuke Iwasawa},\n  eprint = {2205.11916},\n  primaryClass = {cs.CL},\n  title = {Large Language Models are Zero-Shot Reasoners},\n  url = {https://arxiv.org/abs/2205.11916},\n  year = {2023}\n}",
  "CoT-Valve: Length-Compressible Chain-of-Thought Tuning": "@misc{ma2025cotvalvelengthcompressiblechainofthoughttuning,\n  archivePrefix = {arXiv},\n  author = {Xinyin Ma and Guangnian Wan and Runpeng Yu and Gongfan Fang and Xinchao Wang},\n  eprint = {2502.09601},\n  primaryClass = {cs.AI},\n  title = {CoT-Valve: Length-Compressible Chain-of-Thought Tuning},\n  url = {https://arxiv.org/abs/2502.09601},\n  year = {2025}\n}",
  "Compressed Chain of Thought: Efficient Reasoning Through Dense Representations": "@misc{cheng2024compressedchainthoughtefficient,\n  archivePrefix = {arXiv},\n  author = {Jeffrey Cheng and Benjamin Van Durme},\n  eprint = {2412.13171},\n  primaryClass = {cs.CL},\n  title = {Compressed Chain of Thought: Efficient Reasoning Through Dense Representations},\n  url = {https://arxiv.org/abs/2412.13171},\n  year = {2024}\n}",
  "Constitutional AI: Harmlessness from AI Feedback": "@misc{bai2022constitutionalaiharmlessnessai,\n  archivePrefix = {arXiv},\n  author = {Yuntao Bai and Saurav Kadavath and Sandipan Kundu and Amanda Askell and Jackson Kernion and Andy Jones and Anna Chen and Anna Goldie and Azalia Mirhoseini and Cameron McKinnon and Carol Chen and Catherine Olsson and Christopher Olah and Danny Hernandez and Dawn Drain and Deep Ganguli and Dustin Li and Eli Tran-Johnson and Ethan Perez and Jamie Kerr and Jared Mueller and Jeffrey Ladish and Joshua Landau and Kamal Ndousse and Kamile Lukosuite and Liane Lovitt and Michael Sellitto and Nelson Elhage and Nicholas Schiefer and Noemi Mercado and Nova DasSarma and Robert Lasenby and Robin Larson and Sam Ringer and Scott Johnston and Shauna Kravec and Sheer El Showk and Stanislav Fort and Tamera Lanham and Timothy Telleen-Lawton and Tom Conerly and Tom Henighan and Tristan Hume and Samuel R. Bowman and Zac Hatfield-Dodds and Ben Mann and Dario Amodei and Nicholas Joseph and Sam McCandlish and Tom Brown and Jared Kaplan},\n  eprint = {2212.08073},\n  primaryClass = {cs.CL},\n  title = {Constitutional AI: Harmlessness from AI Feedback},\n  url = {https://arxiv.org/abs/2212.08073},\n  year = {2022}\n}",
  "Controllable Safety Alignment: Inference-Time Adaptation to Diverse Safety Requirements": "@misc{zhang2025controllablesafetyalignmentinferencetime,\n  archivePrefix = {arXiv},\n  author = {Jingyu Zhang and Ahmed Elgohary and Ahmed Magooda and Daniel Khashabi and Benjamin Van Durme},\n  eprint = {2410.08968},\n  primaryClass = {cs.CL},\n  title = {Controllable Safety Alignment: Inference-Time Adaptation to Diverse Safety Requirements},\n  url = {https://arxiv.org/abs/2410.08968},\n  year = {2025}\n}",
  "Early Stopping Chain-of-thoughts in Large Language Models": "@misc{mao2025earlystoppingchainofthoughtslarge,\n  archivePrefix = {arXiv},\n  author = {Minjia Mao and Bowen Yin and Yu Zhu and Xiao Fang},\n  eprint = {2509.14004},\n  primaryClass = {cs.CL},\n  title = {Early Stopping Chain-of-thoughts in Large Language Models},\n  url = {https://arxiv.org/abs/2509.14004},\n  year = {2025}\n}",
  "FIRE: Fact-checking with Iterative Retrieval and Verification": "@inproceedings{Xie_2025,\n  author = {Xie, Zhuohan and Xing, Rui and Wang, Yuxia and Geng, Jiahui and Iqbal, Hasan and Sahnan, Dhruv and Gurevych, Iryna and Nakov, Preslav},\n  booktitle = {Findings of the Association for Computational Linguistics: NAACL 2025},\n  DOI = {10.18653/v1/2025.findings-naacl.158},\n  pages = {2901–2914},\n  publisher = {Association for Computational Linguistics},\n  title = {FIRE: Fact-checking with Iterative Retrieval and Verification},\n  url = {http://dx.doi.org/10.18653/v1/2025.findings-naacl.158},\n  year = {2025}\n}",
  "Graph-based Confidence Calibration for Large Language Models": "@article{Virtanen_2020,\n  author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and van der Walt, Stéfan J. and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C J and Polat, İlhan and Feng, Yu and Moore, Eric W. and VanderPlas, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Antônio H. and Pedregosa, Fabian and van Mulbregt, Paul and Vijaykumar, Aditya and Bardelli, Alessandro Pietro and Rothberg, Alex and Hilboll, Andreas and Kloeckner, Andreas and Scopatz, Anthony and Lee, Antony and Rokem, Ariel and Woods, C. Nathan and Fulton, Chad and Masson, Charles and Häggström, Christian and Fitzgerald, Clark and Nicholson, David A. and Hagen, David R. and Pasechnik, Dmitrii V. and Olivetti, Emanuele and Martin, Eric and Wieser, Eric and Silva, Fabrice and Lenders, Felix and Wilhelm, Florian and Young, G. and Price, Gavin A. and Ingold, Gert-Ludwig and Allen, Gregory E. and Lee, Gregory R. and Audren, Hervé and Probst, Irvin and Dietrich, Jörg P. and Silterra, Jacob and Webber, James T and Slavič, Janko and Nothman, Joel and Buchner, Johannes and Kulick, Johannes and Schönberger, Johannes L. and de Miranda Cardoso, José Vinícius and Reimer, Joscha and Harrington, Joseph and Rodríguez, Juan Luis Cano and Nunez-Iglesias, Juan and Kuczynski, Justin and Tritz, Kevin and Thoma, Martin and Newville, Matthew and Kümmerer, Matthias and Bolingbroke, Maximilian and Tartre, Michael and Pak, Mikhail and Smith, Nathaniel J. and Nowaczyk, Nikolai and Shebanov, Nikolay and Pavlyk, Oleksandr and Brodtkorb, Per A. and Lee, Perry and McGibbon, Robert T. and Feldbauer, Roman and Lewis, Sam and Tygier, Sam and Sievert, Scott and Vigna, Sebastiano and Peterson, Stefan and More, Surhud and Pudlik, Tadeusz and Oshima, Takuya and Pingel, Thomas J. and Robitaille, Thomas P. and Spura, Thomas and Jones, Thouis R. and Cera, Tim and Leslie, Tim and Zito, Tiziano and Krauss, Tom and Upadhyay, Utkarsh and Halchenko, Yaroslav O. and Vázquez-Baeza, Yoshiki},\n  DOI = {10.1038/s41592-019-0686-2},\n  ISSN = {1548-7105},\n  journal = {Nature Methods},\n  month = feb,\n  number = {3},\n  pages = {261–272},\n  publisher = {Springer Science and Business Media LLC},\n  title = {SciPy 1.0: fundamental algorithms for scientific computing in Python},\n  url = {http://dx.doi.org/10.1038/s41592-019-0686-2},\n  volume = {17},\n  year = {2020}\n}",
  "HALT-CoT: Model-Agnostic Early Stopping for Chain-of-Thought Reasoning via Answer Entropy": "@inproceedings{Aggarwal_2023,\n  author = {Aggarwal, Pranjal and Madaan, Aman and Yang, Yiming and Mausam},\n  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},\n  DOI = {10.18653/v1/2023.emnlp-main.761},\n  pages = {12375–12396},\n  publisher = {Association for Computational Linguistics},\n  title = {Let’s Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs},\n  url = {http://dx.doi.org/10.18653/v1/2023.emnlp-main.761},\n  year = {2023}\n}",
  "Implicit Chain of Thought Reasoning via Knowledge Distillation": "@misc{deng2023implicitchainthoughtreasoning,\n  archivePrefix = {arXiv},\n  author = {Yuntian Deng and Kiran Prasad and Roland Fernandez and Paul Smolensky and Vishrav Chaudhary and Stuart Shieber},\n  eprint = {2311.01460},\n  primaryClass = {cs.CL},\n  title = {Implicit Chain of Thought Reasoning via Knowledge Distillation},\n  url = {https://arxiv.org/abs/2311.01460},\n  year = {2023}\n}",
  "Improving Large Language Models Function Calling and Interpretability via Guided-Structured Templates": "@article{Virtanen_2020,\n  author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and van der Walt, Stéfan J. and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C J and Polat, İlhan and Feng, Yu and Moore, Eric W. and VanderPlas, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Antônio H. and Pedregosa, Fabian and van Mulbregt, Paul and Vijaykumar, Aditya and Bardelli, Alessandro Pietro and Rothberg, Alex and Hilboll, Andreas and Kloeckner, Andreas and Scopatz, Anthony and Lee, Antony and Rokem, Ariel and Woods, C. Nathan and Fulton, Chad and Masson, Charles and Häggström, Christian and Fitzgerald, Clark and Nicholson, David A. and Hagen, David R. and Pasechnik, Dmitrii V. and Olivetti, Emanuele and Martin, Eric and Wieser, Eric and Silva, Fabrice and Lenders, Felix and Wilhelm, Florian and Young, G. and Price, Gavin A. and Ingold, Gert-Ludwig and Allen, Gregory E. and Lee, Gregory R. and Audren, Hervé and Probst, Irvin and Dietrich, Jörg P. and Silterra, Jacob and Webber, James T and Slavič, Janko and Nothman, Joel and Buchner, Johannes and Kulick, Johannes and Schönberger, Johannes L. and de Miranda Cardoso, José Vinícius and Reimer, Joscha and Harrington, Joseph and Rodríguez, Juan Luis Cano and Nunez-Iglesias, Juan and Kuczynski, Justin and Tritz, Kevin and Thoma, Martin and Newville, Matthew and Kümmerer, Matthias and Bolingbroke, Maximilian and Tartre, Michael and Pak, Mikhail and Smith, Nathaniel J. and Nowaczyk, Nikolai and Shebanov, Nikolay and Pavlyk, Oleksandr and Brodtkorb, Per A. and Lee, Perry and McGibbon, Robert T. and Feldbauer, Roman and Lewis, Sam and Tygier, Sam and Sievert, Scott and Vigna, Sebastiano and Peterson, Stefan and More, Surhud and Pudlik, Tadeusz and Oshima, Takuya and Pingel, Thomas J. and Robitaille, Thomas P. and Spura, Thomas and Jones, Thouis R. and Cera, Tim and Leslie, Tim and Zito, Tiziano and Krauss, Tom and Upadhyay, Utkarsh and Halchenko, Yaroslav O. and Vázquez-Baeza, Yoshiki},\n  DOI = {10.1038/s41592-019-0686-2},\n  ISSN = {1548-7105},\n  journal = {Nature Methods},\n  month = feb,\n  number = {3},\n  pages = {261–272},\n  publisher = {Springer Science and Business Media LLC},\n  title = {SciPy 1.0: fundamental algorithms for scientific computing in Python},\n  url = {http://dx.doi.org/10.1038/s41592-019-0686-2},\n  volume = {17},\n  year = {2020}\n}",
  "Investigating Continual Pretraining in Large Language Models: Insights and Implications": "@misc{yıldız2025investigatingcontinualpretraininglarge,\n  archivePrefix = {arXiv},\n  author = {Çağatay Yıldız and Nishaanth Kanna Ravichandran and Nitin Sharma and Matthias Bethge and Beyza Ermis},\n  eprint = {2402.17400},\n  primaryClass = {cs.CL},\n  title = {Investigating Continual Pretraining in Large Language Models: Insights and Implications},\n  url = {https://arxiv.org/abs/2402.17400},\n  year = {2025}\n}",
  "Kimi k1.5: Scaling Reinforcement Learning with LLMs": "@misc{kimiteam2025kimik15scalingreinforcement,\n  archivePrefix = {arXiv},\n  author = {Kimi Team and Angang Du and Bofei Gao and Bowei Xing and Changjiu Jiang and Cheng Chen and Cheng Li and Chenjun Xiao and Chenzhuang Du and Chonghua Liao and Chuning Tang and Congcong Wang and Dehao Zhang and Enming Yuan and Enzhe Lu and Fengxiang Tang and Flood Sung and Guangda Wei and Guokun Lai and Haiqing Guo and Han Zhu and Hao Ding and Hao Hu and Hao Yang and Hao Zhang and Haotian Yao and Haotian Zhao and Haoyu Lu and Haoze Li and Haozhen Yu and Hongcheng Gao and Huabin Zheng and Huan Yuan and Jia Chen and Jianhang Guo and Jianlin Su and Jianzhou Wang and Jie Zhao and Jin Zhang and Jingyuan Liu and Junjie Yan and Junyan Wu and Lidong Shi and Ling Ye and Longhui Yu and Mengnan Dong and Neo Zhang and Ningchen Ma and Qiwei Pan and Qucheng Gong and Shaowei Liu and Shengling Ma and Shupeng Wei and Sihan Cao and Siying Huang and Tao Jiang and Weihao Gao and Weimin Xiong and Weiran He and Weixiao Huang and Weixin Xu and Wenhao Wu and Wenyang He and Xianghui Wei and Xianqing Jia and Xingzhe Wu and Xinran Xu and Xinxing Zu and Xinyu Zhou and Xuehai Pan and Y. Charles and Yang Li and Yangyang Hu and Yangyang Liu and Yanru Chen and Yejie Wang and Yibo Liu and Yidao Qin and Yifeng Liu and Ying Yang and Yiping Bao and Yulun Du and Yuxin Wu and Yuzhi Wang and Zaida Zhou and Zhaoji Wang and Zhaowei Li and Zhen Zhu and Zheng Zhang and Zhexu Wang and Zhilin Yang and Zhiqi Huang and Zihao Huang and Ziyao Xu and Zonghan Yang and Zongyu Lin},\n  eprint = {2501.12599},\n  primaryClass = {cs.AI},\n  title = {Kimi k1.5: Scaling Reinforcement Learning with LLMs},\n  url = {https://arxiv.org/abs/2501.12599},\n  year = {2025}\n}",
  "LLM-Cite: Cheap Fact Verification with Attribution via URL Generation": "@article{Kanbach_2023,\n  author = {Kanbach, Dominik K. and Heiduk, Louisa and Blueher, Georg and Schreiter, Maximilian and Lahmann, Alexander},\n  DOI = {10.1007/s11846-023-00696-z},\n  ISSN = {1863-6691},\n  journal = {Review of Managerial Science},\n  month = sep,\n  number = {4},\n  pages = {1189–1220},\n  publisher = {Springer Science and Business Media LLC},\n  title = {The GenAI is out of the bottle: generative artificial intelligence from a business model innovation perspective},\n  url = {http://dx.doi.org/10.1007/s11846-023-00696-z},\n  volume = {18},\n  year = {2023}\n}",
  "Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models": "@misc{zhou2024languageagenttreesearch,\n  archivePrefix = {arXiv},\n  author = {Andy Zhou and Kai Yan and Michal Shlapentokh-Rothman and Haohan Wang and Yu-Xiong Wang},\n  eprint = {2310.04406},\n  primaryClass = {cs.AI},\n  title = {Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models},\n  url = {https://arxiv.org/abs/2310.04406},\n  year = {2024}\n}",
  "Large Language Models Meet Symbolic Provers for Logical Reasoning Evaluation": "@misc{qi2025largelanguagemodelsmeet,\n  archivePrefix = {arXiv},\n  author = {Chengwen Qi and Ren Ma and Bowen Li and He Du and Binyuan Hui and Jinwang Wu and Yuanjun Laili and Conghui He},\n  eprint = {2502.06563},\n  primaryClass = {cs.CL},\n  title = {Large Language Models Meet Symbolic Provers for Logical Reasoning Evaluation},\n  url = {https://arxiv.org/abs/2502.06563},\n  year = {2025}\n}",
  "Large Language Models are Zero-Shot Reasoners": "@misc{kojima2023largelanguagemodelszeroshot,\n  archivePrefix = {arXiv},\n  author = {Takeshi Kojima and Shixiang Shane Gu and Machel Reid and Yutaka Matsuo and Yusuke Iwasawa},\n  eprint = {2205.11916},\n  primaryClass = {cs.CL},\n  title = {Large Language Models are Zero-Shot Reasoners},\n  url = {https://arxiv.org/abs/2205.11916},\n  year = {2023}\n}",
  "Large Language Monkeys: Scaling Inference Compute with Repeated Sampling": "@misc{brown2024largelanguagemonkeysscaling,\n  archivePrefix = {arXiv},\n  author = {Bradley Brown and Jordan Juravsky and Ryan Ehrlich and Ronald Clark and Quoc V. Le and Christopher Ré and Azalia Mirhoseini},\n  eprint = {2407.21787},\n  primaryClass = {cs.LG},\n  title = {Large Language Monkeys: Scaling Inference Compute with Repeated Sampling},\n  url = {https://arxiv.org/abs/2407.21787},\n  year = {2024}\n}",
  "Learning diverse attacks on large language models for robust red-teaming and safety tuning": "@misc{lee2025learningdiverseattackslarge,\n  archivePrefix = {arXiv},\n  author = {Seanie Lee and Minsu Kim and Lynn Cherif and David Dobre and Juho Lee and Sung Ju Hwang and Kenji Kawaguchi and Gauthier Gidel and Yoshua Bengio and Nikolay Malkin and Moksh Jain},\n  eprint = {2405.18540},\n  primaryClass = {cs.CL},\n  title = {Learning diverse attacks on large language models for robust red-teaming and safety tuning},\n  url = {https://arxiv.org/abs/2405.18540},\n  year = {2025}\n}",
  "Let LRMs Break Free from Overthinking via Self-Braking Tuning": "@misc{zhao2025letlrmsbreakfree,\n  archivePrefix = {arXiv},\n  author = {Haoran Zhao and Yuchen Yan and Yongliang Shen and Haolei Xu and Wenqi Zhang and Kaitao Song and Jian Shao and Weiming Lu and Jun Xiao and Yueting Zhuang},\n  eprint = {2505.14604},\n  primaryClass = {cs.CL},\n  title = {Let LRMs Break Free from Overthinking via Self-Braking Tuning},\n  url = {https://arxiv.org/abs/2505.14604},\n  year = {2025}\n}",
  "Let's Verify Step by Step": "@misc{lightman2023letsverifystepstep,\n  archivePrefix = {arXiv},\n  author = {Hunter Lightman and Vineet Kosaraju and Yura Burda and Harri Edwards and Bowen Baker and Teddy Lee and Jan Leike and John Schulman and Ilya Sutskever and Karl Cobbe},\n  eprint = {2305.20050},\n  primaryClass = {cs.LG},\n  title = {Let's Verify Step by Step},\n  url = {https://arxiv.org/abs/2305.20050},\n  year = {2023}\n}",
  "LogiCoT: Logical Chain-of-Thought Instruction Tuning": "@inproceedings{Liu_2023,\n  author = {Liu, Hanmeng and Teng, Zhiyang and Cui, Leyang and Zhang, Chaoli and Zhou, Qiji and Zhang, Yue},\n  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2023},\n  DOI = {10.18653/v1/2023.findings-emnlp.191},\n  pages = {2908–2921},\n  publisher = {Association for Computational Linguistics},\n  title = {LogiCoT: Logical Chain-of-Thought Instruction Tuning},\n  url = {http://dx.doi.org/10.18653/v1/2023.findings-emnlp.191},\n  year = {2023}\n}",
  "Model Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation": "@inproceedings{Qi_2024,\n  author = {Qi, Jirui and Sarti, Gabriele and Fernández, Raquel and Bisazza, Arianna},\n  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},\n  DOI = {10.18653/v1/2024.emnlp-main.347},\n  pages = {6037–6053},\n  publisher = {Association for Computational Linguistics},\n  title = {Model Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation},\n  url = {http://dx.doi.org/10.18653/v1/2024.emnlp-main.347},\n  year = {2024}\n}",
  "Multilingual Jailbreak Challenges in Large Language Models": "@misc{deng2024multilingualjailbreakchallengeslarge,\n  archivePrefix = {arXiv},\n  author = {Yue Deng and Wenxuan Zhang and Sinno Jialin Pan and Lidong Bing},\n  eprint = {2310.06474},\n  primaryClass = {cs.CL},\n  title = {Multilingual Jailbreak Challenges in Large Language Models},\n  url = {https://arxiv.org/abs/2310.06474},\n  year = {2024}\n}",
  "OpenAI Code Interpreter": "@inproceedings{Firdous_2023,\n  author = {Firdous, Faisal and Bashir, Saimul and Rufai, Syed Zoofa and Kumar, Sanjeev},\n  booktitle = {2023 2nd International Conference on Edge Computing and Applications (ICECAA)},\n  DOI = {10.1109/icecaa58104.2023.10212275},\n  month = jul,\n  pages = {1192–1197},\n  publisher = {IEEE},\n  title = {OpenAI ChatGPT as a Logical Interpreter of code},\n  url = {http://dx.doi.org/10.1109/icecaa58104.2023.10212275},\n  year = {2023}\n}",
  "Perspective API, Detoxify": "@article{Engel_2013,\n  author = {Engel, Philipp and Moran, Nancy A.},\n  DOI = {10.1111/1574-6976.12025},\n  ISSN = {1574-6976},\n  journal = {FEMS Microbiology Reviews},\n  month = sep,\n  number = {5},\n  pages = {699–735},\n  publisher = {Oxford University Press (OUP)},\n  title = {The gut microbiota of insects – diversity in structure and function},\n  url = {http://dx.doi.org/10.1111/1574-6976.12025},\n  volume = {37},\n  year = {2013}\n}",
  "REALM: Retrieval-Augmented Language Model Pre-Training": "@misc{guu2020realmretrievalaugmentedlanguagemodel,\n  archivePrefix = {arXiv},\n  author = {Kelvin Guu and Kenton Lee and Zora Tung and Panupong Pasupat and Ming-Wei Chang},\n  eprint = {2002.08909},\n  primaryClass = {cs.CL},\n  title = {REALM: Retrieval-Augmented Language Model Pre-Training},\n  url = {https://arxiv.org/abs/2002.08909},\n  year = {2020}\n}",
  "ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search": "@misc{zhang2024restmctsllmselftrainingprocess,\n  archivePrefix = {arXiv},\n  author = {Dan Zhang and Sining Zhoubian and Ziniu Hu and Yisong Yue and Yuxiao Dong and Jie Tang},\n  eprint = {2406.03816},\n  primaryClass = {cs.CL},\n  title = {ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search},\n  url = {https://arxiv.org/abs/2406.03816},\n  year = {2024}\n}",
  "ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search, rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking": "@inproceedings{Zhai_2025,\n  author = {Zhai, Yuanzhao and Liu, Huanxi and Zhang, Zhuo and Lin, Tong and Xu, Kele and Yang, Cheng and Feng, Dawei and Ding, Bo and Wang, Huaimin},\n  booktitle = {Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval},\n  collection = {SIGIR ’25},\n  DOI = {10.1145/3726302.3729965},\n  month = jul,\n  pages = {1444–1454},\n  publisher = {ACM},\n  series = {SIGIR ’25},\n  title = {Empowering Large Language Model Agent through Step-Level Self-Critique and Self-Training},\n  url = {http://dx.doi.org/10.1145/3726302.3729965},\n  year = {2025}\n}",
  "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, Retrieval-Augmented Generation for Large Language Models: A Survey": "@article{Alzubaidi_2021,\n  author = {Alzubaidi, Laith and Zhang, Jinglan and Humaidi, Amjad J. and Al-Dujaili, Ayad and Duan, Ye and Al-Shamma, Omran and Santamaría, J. and Fadhel, Mohammed A. and Al-Amidie, Muthana and Farhan, Laith},\n  DOI = {10.1186/s40537-021-00444-8},\n  ISSN = {2196-1115},\n  journal = {Journal of Big Data},\n  month = mar,\n  number = {1},\n  publisher = {Springer Science and Business Media LLC},\n  title = {Review of deep learning: concepts, CNN architectures, challenges, applications, future directions},\n  url = {http://dx.doi.org/10.1186/s40537-021-00444-8},\n  volume = {8},\n  year = {2021}\n}",
  "Retrieval-Augmented Generation for Large Language Models: A Survey": "@misc{gao2024retrievalaugmentedgenerationlargelanguage,\n  archivePrefix = {arXiv},\n  author = {Yunfan Gao and Yun Xiong and Xinyu Gao and Kangxiang Jia and Jinliu Pan and Yuxi Bi and Yi Dai and Jiawei Sun and Meng Wang and Haofen Wang},\n  eprint = {2312.10997},\n  primaryClass = {cs.CL},\n  title = {Retrieval-Augmented Generation for Large Language Models: A Survey},\n  url = {https://arxiv.org/abs/2312.10997},\n  year = {2024}\n}",
  "Robust LLM safeguarding via refusal feature adversarial training": "@misc{yu2025robustllmsafeguardingrefusal,\n  archivePrefix = {arXiv},\n  author = {Lei Yu and Virginie Do and Karen Hambardzumyan and Nicola Cancedda},\n  eprint = {2409.20089},\n  primaryClass = {cs.LG},\n  title = {Robust LLM safeguarding via refusal feature adversarial training},\n  url = {https://arxiv.org/abs/2409.20089},\n  year = {2025}\n}",
  "Robust Safety Classifier for Large Language Models: Adversarial Prompt Shield": "@misc{kim2023robustsafetyclassifierlarge,\n  archivePrefix = {arXiv},\n  author = {Jinhwa Kim and Ali Derakhshan and Ian G. Harris},\n  eprint = {2311.00172},\n  primaryClass = {cs.CL},\n  title = {Robust Safety Classifier for Large Language Models: Adversarial Prompt Shield},\n  url = {https://arxiv.org/abs/2311.00172},\n  year = {2023}\n}",
  "Rule Based Rewards for Language Model Safety": "@misc{mu2024rulebasedrewardslanguage,\n  archivePrefix = {arXiv},\n  author = {Tong Mu and Alec Helyar and Johannes Heidecke and Joshua Achiam and Andrea Vallone and Ian Kivlichan and Molly Lin and Alex Beutel and John Schulman and Lilian Weng},\n  eprint = {2411.01111},\n  primaryClass = {cs.AI},\n  title = {Rule Based Rewards for Language Model Safety},\n  url = {https://arxiv.org/abs/2411.01111},\n  year = {2024}\n}",
  "Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters": "@misc{snell2024scalingllmtesttimecompute,\n  archivePrefix = {arXiv},\n  author = {Charlie Snell and Jaehoon Lee and Kelvin Xu and Aviral Kumar},\n  eprint = {2408.03314},\n  primaryClass = {cs.LG},\n  title = {Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters},\n  url = {https://arxiv.org/abs/2408.03314},\n  year = {2024}\n}",
  "Self-Consistency Improves Chain of Thought Reasoning in Language Models": "@misc{wang2023selfconsistencyimproveschainthought,\n  archivePrefix = {arXiv},\n  author = {Xuezhi Wang and Jason Wei and Dale Schuurmans and Quoc Le and Ed Chi and Sharan Narang and Aakanksha Chowdhery and Denny Zhou},\n  eprint = {2203.11171},\n  primaryClass = {cs.CL},\n  title = {Self-Consistency Improves Chain of Thought Reasoning in Language Models},\n  url = {https://arxiv.org/abs/2203.11171},\n  year = {2023}\n}",
  "Self-Refine: Iterative Refinement with Self-Feedback": "@misc{madaan2023selfrefineiterativerefinementselffeedback,\n  archivePrefix = {arXiv},\n  author = {Aman Madaan and Niket Tandon and Prakhar Gupta and Skyler Hallinan and Luyu Gao and Sarah Wiegreffe and Uri Alon and Nouha Dziri and Shrimai Prabhumoye and Yiming Yang and Shashank Gupta and Bodhisattwa Prasad Majumder and Katherine Hermann and Sean Welleck and Amir Yazdanbakhsh and Peter Clark},\n  eprint = {2303.17651},\n  primaryClass = {cs.CL},\n  title = {Self-Refine: Iterative Refinement with Self-Feedback},\n  url = {https://arxiv.org/abs/2303.17651},\n  year = {2023}\n}",
  "Self-Training Elicits Concise Reasoning in Large Language Models": "@inproceedings{Munkhbat_2025,\n  author = {Munkhbat, Tergel and Ho, Namgyu and Kim, Seo Hyun and Yang, Yongjin and Kim, Yujin and Yun, Se-Young},\n  booktitle = {Findings of the Association for Computational Linguistics: ACL 2025},\n  DOI = {10.18653/v1/2025.findings-acl.1289},\n  pages = {25127–25152},\n  publisher = {Association for Computational Linguistics},\n  title = {Self-Training Elicits Concise Reasoning in Large Language Models},\n  url = {http://dx.doi.org/10.18653/v1/2025.findings-acl.1289},\n  year = {2025}\n}",
  "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models": "@inproceedings{Manakul_2023,\n  author = {Manakul, Potsawee and Liusie, Adian and Gales, Mark},\n  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},\n  DOI = {10.18653/v1/2023.emnlp-main.557},\n  pages = {9004–9017},\n  publisher = {Association for Computational Linguistics},\n  title = {SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models},\n  url = {http://dx.doi.org/10.18653/v1/2023.emnlp-main.557},\n  year = {2023}\n}",
  "SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs": "@inproceedings{Xu_2025,\n  author = {Xu, Yige and Guo, Xu and Zeng, Zhiwei and Miao, Chunyan},\n  booktitle = {Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  DOI = {10.18653/v1/2025.acl-long.1137},\n  pages = {23336–23351},\n  publisher = {Association for Computational Linguistics},\n  title = {SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs},\n  url = {http://dx.doi.org/10.18653/v1/2025.acl-long.1137},\n  year = {2025}\n}",
  "Source-Aware Training Enables Knowledge Attribution in Language Models": "@misc{khalifa2024sourceawaretrainingenablesknowledge,\n  archivePrefix = {arXiv},\n  author = {Muhammad Khalifa and David Wadden and Emma Strubell and Honglak Lee and Lu Wang and Iz Beltagy and Hao Peng},\n  eprint = {2404.01019},\n  primaryClass = {cs.CL},\n  title = {Source-Aware Training Enables Knowledge Attribution in Language Models},\n  url = {https://arxiv.org/abs/2404.01019},\n  year = {2024}\n}",
  "TemplateRL: Structured Template-Guided Reinforcement Learning for LLM Reasoning": "@misc{wu2025templaterlstructuredtemplateguidedreinforcement,\n  archivePrefix = {arXiv},\n  author = {Jinyang Wu and Chonghua Liao and Mingkuan Feng and Shuai Zhang and Zhengqi Wen and Haoran Luo and Ling Yang and Huazhe Xu and Jianhua Tao},\n  eprint = {2505.15692},\n  primaryClass = {cs.CL},\n  title = {TemplateRL: Structured Template-Guided Reinforcement Learning for LLM Reasoning},\n  url = {https://arxiv.org/abs/2505.15692},\n  year = {2025}\n}",
  "TokenSkip: Controllable Chain-of-Thought Compression in LLMs": "@inproceedings{Xia_2025,\n  author = {Xia, Heming and Leong, Chak Tou and Wang, Wenjie and Li, Yongqi and Li, Wenjie},\n  booktitle = {Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing},\n  DOI = {10.18653/v1/2025.emnlp-main.165},\n  pages = {3351–3363},\n  publisher = {Association for Computational Linguistics},\n  title = {TokenSkip: Controllable Chain-of-Thought Compression in LLMs},\n  url = {http://dx.doi.org/10.18653/v1/2025.emnlp-main.165},\n  year = {2025}\n}",
  "Tool-MAD: A Multi-Agent Debate Framework for Fact Verification with Diverse Tool Augmentation and Adaptive Retrieval": "@misc{jeong2026toolmadmultiagentdebateframework,\n  archivePrefix = {arXiv},\n  author = {Seyeon Jeong and Yeonjun Choi and JongWook Kim and Beakcheol Jang},\n  eprint = {2601.04742},\n  primaryClass = {cs.CL},\n  title = {Tool-MAD: A Multi-Agent Debate Framework for Fact Verification with Diverse Tool Augmentation and Adaptive Retrieval},\n  url = {https://arxiv.org/abs/2601.04742},\n  year = {2026}\n}",
  "Toolformer: Language Models Can Teach Themselves to Use Tools": "@misc{schick2023toolformerlanguagemodelsteach,\n  archivePrefix = {arXiv},\n  author = {Timo Schick and Jane Dwivedi-Yu and Roberto Dessì and Roberta Raileanu and Maria Lomeli and Luke Zettlemoyer and Nicola Cancedda and Thomas Scialom},\n  eprint = {2302.04761},\n  primaryClass = {cs.CL},\n  title = {Toolformer: Language Models Can Teach Themselves to Use Tools},\n  url = {https://arxiv.org/abs/2302.04761},\n  year = {2023}\n}",
  "Train for Truth, Keep the Skills: Binary Retrieval-Augmented Reward Mitigates Hallucinations": "@misc{chen2025traintruthskillsbinary,\n  archivePrefix = {arXiv},\n  author = {Tong Chen and Akari Asai and Luke Zettlemoyer and Hannaneh Hajishirzi and Faeze Brahman},\n  eprint = {2510.17733},\n  primaryClass = {cs.CL},\n  title = {Train for Truth, Keep the Skills: Binary Retrieval-Augmented Reward Mitigates Hallucinations},\n  url = {https://arxiv.org/abs/2510.17733},\n  year = {2025}\n}",
  "Training Large Language Models to Reason in a Continuous Latent Space": "@article{Alzubaidi_2021,\n  author = {Alzubaidi, Laith and Zhang, Jinglan and Humaidi, Amjad J. and Al-Dujaili, Ayad and Duan, Ye and Al-Shamma, Omran and Santamaría, J. and Fadhel, Mohammed A. and Al-Amidie, Muthana and Farhan, Laith},\n  DOI = {10.1186/s40537-021-00444-8},\n  ISSN = {2196-1115},\n  journal = {Journal of Big Data},\n  month = mar,\n  number = {1},\n  publisher = {Springer Science and Business Media LLC},\n  title = {Review of deep learning: concepts, CNN architectures, challenges, applications, future directions},\n  url = {http://dx.doi.org/10.1186/s40537-021-00444-8},\n  volume = {8},\n  year = {2021}\n}",
  "Tree of Thoughts: Deliberate Problem Solving with Large Language Models": "@misc{yao2023treethoughtsdeliberateproblem,\n  archivePrefix = {arXiv},\n  author = {Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and Thomas L. Griffiths and Yuan Cao and Karthik Narasimhan},\n  eprint = {2305.10601},\n  primaryClass = {cs.CL},\n  title = {Tree of Thoughts: Deliberate Problem Solving with Large Language Models},\n  url = {https://arxiv.org/abs/2305.10601},\n  year = {2023}\n}",
  "Uncertainty Quantification and Confidence Calibration in Large Language Models": "@inproceedings{Liu_2025,\n  author = {Liu, Xiaoou and Chen, Tiejin and Da, Longchao and Chen, Chacha and Lin, Zhen and Wei, Hua},\n  booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2},\n  collection = {KDD ’25},\n  DOI = {10.1145/3711896.3736569},\n  month = aug,\n  pages = {6107–6117},\n  publisher = {ACM},\n  series = {KDD ’25},\n  title = {Uncertainty Quantification and Confidence Calibration in Large Language Models: A Survey},\n  url = {http://dx.doi.org/10.1145/3711896.3736569},\n  year = {2025}\n}",
  "Uncertainty Quantification and Confidence Calibration in Large Language Models: A Survey": "@inproceedings{Liu_2025,\n  author = {Liu, Xiaoou and Chen, Tiejin and Da, Longchao and Chen, Chacha and Lin, Zhen and Wei, Hua},\n  booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2},\n  collection = {KDD ’25},\n  DOI = {10.1145/3711896.3736569},\n  month = aug,\n  pages = {6107–6117},\n  publisher = {ACM},\n  series = {KDD ’25},\n  title = {Uncertainty Quantification and Confidence Calibration in Large Language Models: A Survey},\n  url = {http://dx.doi.org/10.1145/3711896.3736569},\n  year = {2025}\n}",
  "VerityMath: Advancing Mathematical Reasoning by Self-Verification Through Unit Consistency": "@misc{han2024veritymathadvancingmathematicalreasoning,\n  archivePrefix = {arXiv},\n  author = {Vernon Toh Yan Han and Ratish Puduppully and Nancy F. Chen},\n  eprint = {2311.07172},\n  primaryClass = {cs.CL},\n  title = {VerityMath: Advancing Mathematical Reasoning by Self-Verification Through Unit Consistency},\n  url = {https://arxiv.org/abs/2311.07172},\n  year = {2024}\n}",
  "rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking": "@misc{guan2025rstarmathsmallllmsmaster,\n  archivePrefix = {arXiv},\n  author = {Xinyu Guan and Li Lyna Zhang and Yifei Liu and Ning Shang and Youran Sun and Yi Zhu and Fan Yang and Mao Yang},\n  eprint = {2501.04519},\n  primaryClass = {cs.CL},\n  title = {rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking},\n  url = {https://arxiv.org/abs/2501.04519},\n  year = {2025}\n}",
  "DAST: Difficulty-Adaptive Slow-Thinking for Large Reasoning Models": "@article{shen2025dast,\n  title={Dast: Difficulty-adaptive slow-thinking for large reasoning models},\n  author={Shen, Yi and Zhang, Jian and Huang, Jieyun and Shi, Shuming and Zhang, Wenjing and Yan, Jiangze and Wang, Ning and Wang, Kai and Liu, Zhaoxiang and Lian, Shiguo},\n  journal={arXiv preprint arXiv:2503.04472},\n  year={2025}\n}",
  "Efficient LLM Safety Evaluation through Multi-Agent Debate": "@article{lin2025efficient,\n  title={Efficient LLM Safety Evaluation through Multi-Agent Debate},\n  author={Lin, Dachuan and Shen, Guobin and Yang, Zihao and Liu, Tianrong and Zhao, Dongcheng and Zeng, Yi},\n  journal={arXiv preprint arXiv:2511.06396},\n  year={2025}\n}",
  "Leash: Adaptive Length Penalty and Reward Shaping for Efficient Large Reasoning Model": "@article{li2025leash,\n  title={Leash: Adaptive Length Penalty and Reward Shaping for Efficient Large Reasoning Model},\n  author={Li, Yanhao and Ma, Lu and Zhang, Jiaran and Tang, Lexiang and Zhang, Wentao and Luo, Guibo},\n  journal={arXiv preprint arXiv:2512.21540},\n  year={2025}\n}",
  "LightThinker: Thinking Step-by-Step Compression": "@article{zhang2025lightthinker,\n  title={Lightthinker: Thinking step-by-step compression},\n  author={Zhang, Jintian and Zhu, Yuqi and Sun, Mengshu and Luo, Yujie and Qiao, Shuofei and Du, Lun and Zheng, Da and Chen, Huajun and Zhang, Ningyu},\n  journal={arXiv preprint arXiv:2502.15589},\n  year={2025}\n}",
  "When Models Outthink Their Safety: Mitigating Self-Jailbreak in Large Reasoning Models with Chain-of-Guardrails": "@article{mao2025models,\n  title={When models outthink their safety: Mitigating self-jailbreak in large reasoning models with chain-of-guardrails},\n  author={Mao, Yingzhi and Zhang, Chunkang and Wang, Junxiang and Guan, Xinyan and Cao, Boxi and Lu, Yaojie and Lin, Hongyu and Han, Xianpei and Sun, Le},\n  year={2025}\n}"
}